\documentclass[a4paper]{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{listings}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{wrapfig}
\usepackage{minted}
\usepackage{makeidx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{subfig}
\usepackage{float}

\pagenumbering{gobble}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\rhead{}
\lhead{\textbf{Introdução a Aprendizado de Máquina}}
\cfoot{\thepage}

\begin{document}
\begin{titlepage}
   \begin{center}
        \vspace{1cm}   
        \LARGE{EEL891 - Introdução a Aprendizado de Máquina \\ 2024/1}

        \vspace{1.5cm}
        \huge{Competição 1}\\
        \vspace{1cm}
     
        \begin{center}
           \includegraphics[width=0.7\textwidth]{Images/minerva.png}
        \end{center}
       
        \vspace{1.5cm}
        %\vspace{2cm}

        \large{
        \item Kaway Henrique da Rocha Marinho - DRE 119056239
        \begin{itemize}
        \item \texttt{kawayrmarinho@poli.ufrj.br}
        \end{itemize}
        }
        
        \vspace{0.5cm}
        \vfill
        
        \vspace{2.3cm}
        Rio de Janeiro - RJ\\
        20 de Julho de 2024 (2024.1)
        
        
   \end{center}
\end{titlepage}

\newpage




\section{Introdução}

O objetivo desse relatório é explorar sobre o código para a competição 2 estabelecido nesse período na disciplina de Introdução a Aprendizado de Máquina. Neste segundo trabalho, são abordados diferentes tipos de regressores, sendo a maioria disponibilizado pela biblioteca scikit-learn e alguns a mais.

\section{Preparação dos dados}

A preparação de dados para esta competição foi feita de maneira mais robusta. Começando pela remoção da coluna diferenciais, que apenas codificava de forma textual as diversas colunas de salão de festa, sauna etc. Além desta, podemos visualizar os dados das colunas categóricas (tipo, tipo\_vendedor, bairro\_preco), verificando a contagem por opção (para verificar a relevância daquela opção) e a média de preço, para verificar a correlação. Geramos os seguintes gráficos:

\begin{figure}[H]
\begin{tabular}{cc}
    \includegraphics[width=75mm]{Images/tipo_vendedor_barplot.png} & \includegraphics[width=75mm]{Images/tipo_vendedor_histplot.png} \\
     (a) Gráfico com o preço médio de tipo\_vendedor & (b) Histograma de tipo\_vendedor  \\ [6pt]
     \includegraphics[width=75mm]{Images/tipo_barplot.png} & \includegraphics[width=75mm]{Images/tipo_vendedor_histplot.png} \\
     (c) Gráfico com o preço médio de tipo & (d) Histograma de tipo \\ [6pt]
\end{tabular}
\caption{Gráficos para tipo de tipo\_vendedor}
\end{figure}

Para a codificação dos dados de uma maneira possível de ser entendida por um modelo de aprendizado, utilizei o One-hot Encoding, utilizando a função fornecida pela biblioteca panda get\_dummies(), que transforma uma coluna de dados categóricos em várias colunas binárias.

\begin{verbatim}
    for dataset in [data, test_data]:
    dummies = pd.get_dummies(dataset['bairro'], prefix = 'bairro')
    dataset[relevant_bairros.index] = dummies[relevant_bairros.index]
\end{verbatim}

Após a codificação, passei para adaptar o restantes das colunas, isso foi feito utilizando o logaritmo (fornecido pelo numpy.log()) das colunas preco, area\_util e area\_extra

\begin{figure}[H]
\begin{tabular}{cc}
    \includegraphics[width=75mm]{Images/area_util_scatterplot.png} & \includegraphics[width=75mm]{Images/area_util_log_scatterplot.png} \\
     (a) Scatterplot de area\_util & (b) Scatterplot de log\_area\_util  \\ [6pt]
     \includegraphics[width=75mm]{Images/area_extra_scatterplot.png} & \includegraphics[width=75mm]{Images/area_extra_log_scatterplot.png} \\
     (c) Scatterplot de area\_extra & (d) Scatterplot de log\_area\_extra \\ [6pt]
\end{tabular}
\caption{Adaptação das colunas de area}
\end{figure}

Por fim, optei por assimilar as features quartos, suites e vagas em duas novas features chamadas comodos, que consiste na soma de quartos e vagas, e razao\_quarto\_vaga, que consiste na razão entre vagas e quartos não suítes. Mais para frente será possível perceber o impacto disso no modelo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/comodos_scatterplot.png}
    \caption{Scatterplot de comodos}
    \label{fig:enter-label}
\end{figure}

\section{Seleção de Atributos}

Os atributos foram selecionados de maneira manual conforme a correlação apresentada. Como o cálculo de correlação foi feito utilizando o coeficiente de pearson, foram escolhidas todas as features com p > 0.10. Isso permite que cada feature tem uma relevância significativa para o modelo utilizado no momento.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/correlations_heatmap.png}
    \caption{Heatmap das correlações}
    \label{fig:enter-label}
\end{figure}

\newpage
\section{Modelos utilizados}

Os modelos utilizados e discutidos foram alguns dos usados em sala de aula bem como a adição dos modelos Xgboost e Catboost, fornecidos por terceiros. Ao fim, foi selecionado o modelo com melhor desempenho, após o ajuste de hiperparâmetros utilizando GridSearchCV

Lista de modelos:

\begin{verbatim}
    LinearRegression
    SGDRegressor
    RandomForestRegressor
    ExtraTreesRegressor
    KNeighborsRegressor
    MLPRegressor
    GradientBoostingRegressor
    LinearSVR
    SVR
    AdaBoostRegressor
    XGBRegressor
\end{verbatim}

\section{Validação}

A validação dos modelos foi feita através do cálculo dos valores de Raiz do Erro Médio Quadrado, Porcentagem da Raiz do Erro Médio Quadrado e ${R^2}$. Utilizei valizadação cruzada com 4 divisões diferentes e extraí essas medidas de cada modelo, conforme mostra o código.

\begin{verbatim}
                    Regressor      RMSE     RMSPE        R2
0            LinearRegression  0.345976  0.026695  0.724267
1                SGDRegressor  0.355517  0.027320  0.708850
2       RandomForestRegressor  0.338519  0.026130  0.736024
3         ExtraTreesRegressor  0.377421  0.029315  0.671868
4         KNeighborsRegressor  0.340356  0.026182  0.733153
5                MLPRegressor  0.348049  0.026883  0.720953
6   GradientBoostingRegressor  0.317654  0.024585  0.767564
7                   LinearSVR  0.347607  0.026796  0.721662
8                         SVR  0.332132  0.025641  0.745893
9           AdaBoostRegressor  0.432472  0.033500  0.569163
10               XGBRegressor  0.346292  0.026921  0.723763
\end{verbatim}

\section{Resultados}

Ao fim do treinamento, optei pelo regressor GradientBoosting, pois performou melhor entre os utilizados e permitiu um ajudes de hiperparâmetros melhor, objetivo que atingi com o GridSearhCV.

\begin{verbatim}
    parameters = {
    # 'loss': ['huber', 'squared_error'],
    # 'learning_rate': [x / 10.0 for x in range(1, 10, 1)],
    # 'criterion': ['friedman_mse', 'squared_error'],
    # 'max_depth': list(range(1, 9)),
    # 'max_features': ['sqrt', 'log2']
              }
grid = GridSearchCV(GradientBoostingRegressor(learning_rate=0.2, loss='huber', max_depth=5, max_features='log2'), 
                    parameters, cv=10)
\end{verbatim}

É importante notar que os paramêtros estão em comentários pois conforme computava cada parametro eu colocava ele como fixo a fim de reduzir o numero de iterações para o ajuste.

Ao fim, obtive o seguinte gráfico de ajuste linear, mostrando que o modelo estava bem otimizado sem chegar ao overfitting

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/real_vs_predito.png}
    \caption{Real X Predito por GradientBoosting}
    \label{fig:enter-label}
\end{figure}

\section{Conclusões}

A competição permitiu uma abordagem mais hands-on do que foi passado em sala de aula, além de permitir uma visualização melhor dos dados aplicados.

\section{Bibliografia}

https://medium.com/@ktoprakucar/how-to-calculate-the-correlation-between-categorical-and-continuous-values-dcb7abf79406

An Introduction to Variable and Feature Selection - Journal of Machine Learning Research 3 (2003) 1157-1182

https://scikit-learn.org/stable/modules/compose.html

https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9

\end{document}

